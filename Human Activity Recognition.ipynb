{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The objective of the project is to train a machine and predict the activity from one of the six activities performed, based on the readings embedded inertial sensors in a waist mounted smartphone. The original dataset is taken from **Kaggle** website under the name *\"Human Activity Recognition with Smartphones\"*.\n",
    "#### Problem Description :\n",
    "The dataset was prepared by 30 volunteers who performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) while wearing a smartphone with embedded accelerometer amd gyroscope. The sensors captured 2-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. Also, it is specified in the dataset description that the sensor signals were pre-processed by applying noise filters and then sampled in fixed-width sliding window of 2.56 sec and 50% overlap(128 readings/window).\n",
    "\n",
    "##### Attribute Information\n",
    "-> Triaxial acceleration from accelerometer<br>\n",
    "-> Triaxial angular velocity from gyroscope<br>\n",
    "-> A 561-feature vector with time and frequency domain variables<br>\n",
    "-> Its activity label<br>\n",
    "-> An human identifier of subject who carried out the experiment<br>\n",
    "\n",
    "Let's start by importing relevant libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df  = pd.read_csv('D:/Users/gajesing/Desktop/Kaggle/human-activity-recognition-with-smartphones/train.csv')\n",
    "test_df  = pd.read_csv('D:/Users/gajesing/Desktop/Kaggle/human-activity-recognition-with-smartphones/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>subject</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>1</td>\n",
       "      <td>STANDING</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X    ...     \\\n",
       "0         -0.923527         -0.934724    ...      \n",
       "1         -0.957686         -0.943068    ...      \n",
       "2         -0.977469         -0.938692    ...      \n",
       "3         -0.989302         -0.938692    ...      \n",
       "4         -0.990441         -0.942469    ...      \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.710304                    -0.112754   \n",
       "1                        -0.861499                     0.053477   \n",
       "2                        -0.760104                    -0.118559   \n",
       "3                        -0.482845                    -0.036788   \n",
       "4                        -0.699205                     0.123320   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.030400                         -0.464761   \n",
       "1                             -0.007435                         -0.732626   \n",
       "2                              0.177899                          0.100699   \n",
       "3                             -0.012892                          0.640011   \n",
       "4                              0.122542                          0.693578   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                             -0.018446             -0.841247   \n",
       "1                              0.703511             -0.844788   \n",
       "2                              0.808529             -0.848933   \n",
       "3                             -0.485366             -0.848649   \n",
       "4                             -0.615971             -0.847865   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  subject  Activity  \n",
       "0              0.179941             -0.058627        1  STANDING  \n",
       "1              0.180289             -0.054317        1  STANDING  \n",
       "2              0.180637             -0.049118        1  STANDING  \n",
       "3              0.181935             -0.047663        1  STANDING  \n",
       "4              0.185151             -0.043892        1  STANDING  \n",
       "\n",
       "[5 rows x 563 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 563), (2947, 563))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z',\n",
       "       'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z',\n",
       "       'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z',\n",
       "       'tBodyAcc-max()-X',\n",
       "       ...\n",
       "       'fBodyBodyGyroJerkMag-kurtosis()', 'angle(tBodyAccMean,gravity)',\n",
       "       'angle(tBodyAccJerkMean),gravityMean)',\n",
       "       'angle(tBodyGyroMean,gravityMean)',\n",
       "       'angle(tBodyGyroJerkMean,gravityMean)', 'angle(X,gravityMean)',\n",
       "       'angle(Y,gravityMean)', 'angle(Z,gravityMean)', 'subject', 'Activity'],\n",
       "      dtype='object', length=563)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traing data has 7352 observations with 563 features and similarly test data has 2947 observations with same features with the first few columns representing the mean and standard deviations of body accelerations in 3 spatial dimensions (X, Y, Z). The last two columns are \"subject\" and \"Acitivity\" in which the subject represent that the observation is taken from and the corresponding activity respectively. Let's see what activities have been recorded in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Labels :\n",
      " ['STANDING' 'SITTING' 'LAYING' 'WALKING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_UPSTAIRS']\n",
      "Test Labels :\n",
      " ['STANDING' 'SITTING' 'LAYING' 'WALKING' 'WALKING_DOWNSTAIRS'\n",
      " 'WALKING_UPSTAIRS']\n"
     ]
    }
   ],
   "source": [
    "print('Training Labels :\\n', train_df.Activity.unique())\n",
    "print('Test Labels :\\n', test_df.Activity.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 activities, 3 passive (laying, standing and sitting) and 3 active (walking, walking_downstairs, walking_upstairs). So, each observation in the dataset represent one of the six activities whose features are recorded in the first 561 variables. Our goal would be to train the machine to predict one of the six activities given a feature set of these 561 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should find if there are any missing or null values, which are not accepted by models. If we find any such values, we have to replace them by using some imputing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set :  0\n",
      "Test set :  0\n"
     ]
    }
   ],
   "source": [
    "print('Training set : ',train_df.isnull().sum().sum())\n",
    "print('Test set : ',test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing data! cool :) So we dont need to do any data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle our training and test datasets to avoid any elements of bias/patterns and make sure that our models remains general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = shuffle(train_df)\n",
    "test_df = shuffle(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to split our dataset into training and test data, because the data which we have got, is already splitted into two sets. So we'll train our models on training set and will test them on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, :-2].values\n",
    "Y_train = train_df.iloc[:, -1].values\n",
    "X_test = test_df.iloc[:, :-2].values\n",
    "Y_test = test_df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 561), (7352,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2947, 561), (2947,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some classification models (like LR, KNN, SVC, MLP) which are sensitive towards the scale of attributes. Though we have almost same scale of all the attributes, still we can normalize our dataset so that it can learn faster. So we'll standardize our dataset based on mean and deviation of training set only. Because we should not touch the test set before fitting the model. The problem of **data leakage** can occur, and can make our model biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using here 9 most frequently used models and evaluating mean accuracy of every cross validation set using stratified k fold method.\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. Support Vector Classifier\n",
    "4. K Nearest Neighbors Classifier\n",
    "5. Gradient Boosting Classifier\n",
    "6. Random Forest Classifier\n",
    "7. AdaBoost Classifier\n",
    "8. Naive Bayes Classifier\n",
    "9. Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are training all the models without tuning parameters and evaluating on validation set. Out of these 9 models, we'll tune the better 4 models and then we'll choose the best out of those 3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('Tree', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('GNB', GaussianNB()))\n",
    "models.append(('Forest', RandomForestClassifier()))\n",
    "models.append(('GBoost', GradientBoostingClassifier()))\n",
    "models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "models.append(('MLP', MLPClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR : 0.985854 (0.002666) in 88.377260 sec.\n",
      "Tree : 0.944369 (0.008752) in 36.274448 sec.\n",
      "KNN : 0.964644 (0.006788) in 35.858841 sec.\n",
      "SVC : 0.978504 (0.005393) in 60.175031 sec.\n",
      "GNB : 0.736361 (0.031642) in 2.026816 sec.\n",
      "Forest : 0.971848 (0.007267) in 9.602464 sec.\n",
      "GBoost : 0.989249 (0.005825) in 2068.713859 sec.\n",
      "AdaBoost : 0.544886 (0.000503) in 217.487000 sec.\n",
      "MLP : 0.985177 (0.003950) in 55.651362 sec.\n"
     ]
    }
   ],
   "source": [
    "names = []\n",
    "scores = []\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits = 10, random_state = 0)\n",
    "    start = time.time()\n",
    "    val_score = cross_val_score(model, X_train_std, Y_train, cv = kfold, scoring = 'accuracy')\n",
    "    end = time.time()\n",
    "    scores.append(val_score)\n",
    "    names.append(name)\n",
    "    print('%s : %f (%f) in %f sec.' %(name, val_score.mean(), val_score.std(), end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that **Logistic Regression**, **Gradient Boosting**, **SVC** and **MLP** classifiers are among the most four accurate models in 10 fold cross validation. So we can tune the parameters for these 4 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - 1. Logistic Regression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.1, 0.3, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "kfold = StratifiedKFold(n_splits = 10, random_state = 0)\n",
    "grid_values = {'penalty' : ['l1', 'l2'], 'C' : [0.1, 0.3, 1]}\n",
    "lr_tuned = GridSearchCV(lr, grid_values, cv = kfold, scoring = 'accuracy', n_jobs = -1)\n",
    "lr_tuned.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878944504896626"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - 2. MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.46919331\n",
      "Iteration 2, loss = 0.11730976\n",
      "Iteration 3, loss = 0.08071720\n",
      "Iteration 4, loss = 0.05950913\n",
      "Iteration 5, loss = 0.05009980\n",
      "Iteration 6, loss = 0.04071802\n",
      "Iteration 7, loss = 0.03651860\n",
      "Iteration 8, loss = 0.03581308\n",
      "Iteration 9, loss = 0.03658988\n",
      "Iteration 10, loss = 0.02573168\n",
      "Iteration 11, loss = 0.02703680\n",
      "Iteration 12, loss = 0.02810736\n",
      "Iteration 13, loss = 0.01804405\n",
      "Iteration 14, loss = 0.01510275\n",
      "Iteration 15, loss = 0.01253899\n",
      "Iteration 16, loss = 0.01030649\n",
      "Iteration 17, loss = 0.01012733\n",
      "Iteration 18, loss = 0.00691303\n",
      "Iteration 19, loss = 0.00544229\n",
      "Iteration 20, loss = 0.00491143\n",
      "Iteration 21, loss = 0.00369778\n",
      "Iteration 22, loss = 0.00314947\n",
      "Iteration 23, loss = 0.00320445\n",
      "Iteration 24, loss = 0.00454467\n",
      "Iteration 25, loss = 0.00314914\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'hidden_layer_sizes': [(100,), (180, 60), (100, 50)], 'activation': ['relu', 'logistic', 'tanh'], 'alpha': [0.0001, 0.0003]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(verbose = True)\n",
    "kfold = StratifiedKFold(n_splits = 10, random_state = 0)\n",
    "grid_values = {'hidden_layer_sizes' : [(100,), (180, 60), (100, 50)], 'activation' : ['relu', 'logistic', 'tanh'], 'alpha' : [0.0001, 0.0003]}\n",
    "mlp_tuned = GridSearchCV(mlp, grid_values, cv = kfold, scoring = 'accuracy', n_jobs = -1)\n",
    "mlp_tuned.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'alpha': 0.0003, 'hidden_layer_sizes': (180, 60)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874863982589771"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_tuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - 3. Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 70.4min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 348.7min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 476.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], 'max_features': ['auto', 'log2', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier()\n",
    "kfold = StratifiedKFold(n_splits = 10, random_state = 0)\n",
    "grid_values = {'n_estimators' : [100, 200, 300], 'max_depth' : [3, 4, 5], 'max_features' : ['auto', 'log2', None]}\n",
    "gboost_tuned = GridSearchCV(gboost, grid_values, cv = kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "gboost_tuned.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'max_features': None, 'n_estimators': 200}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9923830250272034"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost_tuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - 4. SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed: 15.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=0, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'kernel': ['linear', 'rbf'], 'C': [0.03, 0.1, 0.3, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "kfold = StratifiedKFold(n_splits = 10, random_state = 0)\n",
    "grid_values = {'kernel' : ['linear', 'rbf'], 'C' : [0.03, 0.1, 0.3, 1]}\n",
    "svc_tuned = GridSearchCV(svc, grid_values, cv = kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\n",
    "svc_tuned.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9862622415669206"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_tuned.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our three best tuned models and choose one out of four models as final model. Lets' see how accurate are these models on training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set :\n",
      "\n",
      "Logistic Regression :  0.9952393906420022\n",
      "MLP Classifier :  1.0\n",
      "Gradient Boosted Classifier :  1.0\n",
      "SVC :  0.9926550598476604\n",
      "\n",
      "Test Set :\n",
      "\n",
      "Logistic Regression :  0.9599592806243638\n",
      "MLP Classifier :  0.9450288428910757\n",
      "Gradient Boosted Classifier :  0.9314557176789956\n",
      "SVC :  0.9616559212758737\n",
      "\n",
      "Classification Report of Test Set : \n",
      "Logistic Regression \n",
      ":                      precision    recall  f1-score   support\n",
      "\n",
      "            LAYING       1.00      1.00      1.00       537\n",
      "           SITTING       0.97      0.86      0.91       491\n",
      "          STANDING       0.89      0.97      0.93       532\n",
      "           WALKING       0.95      1.00      0.97       496\n",
      "WALKING_DOWNSTAIRS       1.00      0.98      0.99       420\n",
      "  WALKING_UPSTAIRS       0.97      0.95      0.96       471\n",
      "\n",
      "       avg / total       0.96      0.96      0.96      2947\n",
      "\n",
      "MLP Classifier : \n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            LAYING       1.00      0.96      0.98       537\n",
      "           SITTING       0.96      0.87      0.92       491\n",
      "          STANDING       0.86      0.97      0.91       532\n",
      "           WALKING       0.95      0.99      0.97       496\n",
      "WALKING_DOWNSTAIRS       0.98      0.92      0.95       420\n",
      "  WALKING_UPSTAIRS       0.94      0.96      0.95       471\n",
      "\n",
      "       avg / total       0.95      0.95      0.95      2947\n",
      "\n",
      "Gradient Boosted Classifier : \n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "            LAYING       1.00      1.00      1.00       537\n",
      "           SITTING       0.92      0.83      0.87       491\n",
      "          STANDING       0.86      0.93      0.89       532\n",
      "           WALKING       0.94      0.98      0.96       496\n",
      "WALKING_DOWNSTAIRS       0.97      0.91      0.94       420\n",
      "  WALKING_UPSTAIRS       0.91      0.93      0.92       471\n",
      "\n",
      "       avg / total       0.93      0.93      0.93      2947\n",
      "\n",
      "SVC :\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "            LAYING       1.00      1.00      1.00       537\n",
      "           SITTING       0.96      0.89      0.92       491\n",
      "          STANDING       0.91      0.97      0.94       532\n",
      "           WALKING       0.96      1.00      0.98       496\n",
      "WALKING_DOWNSTAIRS       0.99      0.95      0.97       420\n",
      "  WALKING_UPSTAIRS       0.97      0.96      0.96       471\n",
      "\n",
      "       avg / total       0.96      0.96      0.96      2947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training Set :\\n')\n",
    "print('Logistic Regression : ',accuracy_score(Y_train, lr_tuned.predict(X_train_std)))\n",
    "print('MLP Classifier : ',accuracy_score(Y_train, mlp_tuned.predict(X_train_std)))\n",
    "print('Gradient Boosted Classifier : ',accuracy_score(Y_train, gboost_tuned.predict(X_train_std)))\n",
    "print('SVC : ', accuracy_score(Y_train, svc_tuned.predict(X_train_std)))\n",
    "print('\\nTest Set :\\n')\n",
    "print('Logistic Regression : ',accuracy_score(Y_test, lr_tuned.predict(X_test_std)))\n",
    "print('MLP Classifier : ',accuracy_score(Y_test, mlp_tuned.predict(X_test_std)))\n",
    "print('Gradient Boosted Classifier : ',accuracy_score(Y_test, gboost_tuned.predict(X_test_std)))\n",
    "print('SVC : ', accuracy_score(Y_test, svc_tuned.predict(X_test_std)))\n",
    "print('\\nClassification Report of Test Set : ')\n",
    "print('Logistic Regression \\n: ',classification_report(Y_test, lr_tuned.predict(X_test_std)))\n",
    "print('MLP Classifier : \\n',classification_report(Y_test, mlp_tuned.predict(X_test_std)))\n",
    "print('Gradient Boosted Classifier : \\n',classification_report(Y_test, gboost_tuned.predict(X_test_std)))\n",
    "print('SVC :\\n ', classification_report(Y_test, svc_tuned.predict(X_test_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can conclude that **MLP** and **Gradient Boosting** classifiers are better for training set, whereas, **LR** and **SVC** are competitive for training set but better for test set. MLP and GBoost classifier have more variance than LR and SVC.\n",
    "**SVC** with parameters as *C* = 0.1 and *kernel* = linear, is most generalized and accurate model. And by tuning it's parameter we improved it's training accuracy by around 2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our best model **SVC** predicted the training dataset with **99.3%** and test set with **96.2%** accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
